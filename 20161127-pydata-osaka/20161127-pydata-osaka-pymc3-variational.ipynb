{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0\n",
    "%matplotlib inline\n",
    "\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.expanduser('~/git/github/pymc-devs/pymc3'))\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import daft\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "import IPython.core.display as display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "img[src$=\"centerme\"] {\n",
       "  display:block;\n",
       "  margin: 0 auto;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    "img[src$=\"centerme\"] {\n",
    "  display:block;\n",
    "  margin: 0 auto;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br />\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<font size=\"7\"><b>Variational inference in </b></font>\n",
    "<div style=\"display: inline-block; vertical-align: middle;\">\n",
    "<img src=\"./pymc3_logo.jpg\" width=200 height=200 /></div> \n",
    "</div>\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<div style=\"text-align: right;\">\n",
    "<font size=\"6\">Taku Yoshioka</font>\n",
    "</div>\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyMC3\n",
    "* Python library for Bayesian inference\n",
    "\n",
    "    * Define probabilistic models consisting of random variables (RVs)\n",
    "    * Draw samples of RVs from the posterior distribution with MCMC\n",
    "    * __Fit parametrized approximate posterior to data with variational inference__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "* Bayesian inference and variational inference\n",
    "* Stochastic gradient for optimization\n",
    "* Example: Hierarchical linear regression and Bayesian neural network\n",
    "* Autoencoding Variational Bayes (AEVB)\n",
    "* Example: Convolutional variational autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic model\n",
    "* Decomposition of the joint probability of all random variables (RVs)\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x},\\mathbf{z}) = p(\\mathbf{x}|\\mathbf{z})p(\\mathbf{z})\n",
    "$$\n",
    "* $p(\\mathbf{x},\\mathbf{z})$: joint distribution of $\\mathbf{x}$ and $\\mathbf{z}$\n",
    "* $p(\\mathbf{z})$: prior distribution on $\\mathbf{z}$\n",
    "* $p(\\mathbf{x}|\\mathbf{z})$: likelihood of data $\\mathbf{x}$ given $\\mathbf{z}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian inference\n",
    "* Infer the posterior distribution $p(\\mathbf{z}|\\mathbf{x})$, the distribution of the unkown RVs $\\mathbf{z}$ given observations $\\mathbf{x}$ (which is also RVs), based on the Bayes theorem:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{z}|\\mathbf{x}) = \\frac{p(\\mathbf{x},\\mathbf{z})}{p(\\mathbf{x})}=\\frac{p(\\mathbf{x}|\\mathbf{z})p(\\mathbf{z})}{p(\\mathbf{x})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variational inference\n",
    "* Approximate $p(\\mathbf{z}|\\mathbf{x})$ by variational distribution $q(\\mathbf{z})$\n",
    "* Maximize evidence lower bound (ELBO) ${\\cal L}(q)$ w.r.t. $q$ minimizes $KL[q(\\mathbf{z})||p(\\mathbf{z}|\\mathbf{x})]$\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\cal{L}(q) & = & \\mathbb{E}_{q(\\mathbf{z})}\\left[\\log p(\\mathbf{x},\\mathbf{z}) - \\log q(\\mathbf{z})\\right] \\\\\n",
    "           & = & \\mathbb{E}_{q(\\mathbf{z})}\\left[\\log p(\\mathbf{x}|\\mathbf{z})\\right] - KL\\left[\\log q(\\mathbf{z})||p(\\mathbf{z})\\right] \\\\\n",
    "           & = & \\log p(\\mathbf{x}) + KL[q(\\mathbf{z})||p(\\mathbf{z}|\\mathbf{x})]\n",
    "\\end{eqnarray}\n",
    "\n",
    "* Pros: Can obtain tractable solution under factorized $q(\\mathbf{x})$ and conjugate prior\n",
    "* Cons: Restricted probabilistic models because of the tractability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Meanfield approximation\n",
    "* Normal distribution as $q(\\mathbf{z})$ with parameters $\\gamma$ (variational parameters)\n",
    "\n",
    "$$\n",
    "q_{\\lambda}(\\mathbf{z}) = \\prod_{d=1}^{D}N(z_{d};\\mu_{d},\\sigma_{d}^{2}), \\gamma=\\left\\{\\mu_{d}, \\sigma_{d}\\right\\}_{d=1}^{D}\n",
    "$$\n",
    "\n",
    "* Maximize ELBO wrt $\\gamma$ with stochastic gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reparametrization\n",
    "* Reparametrization $\\mathbf{z} = \\mathbf{\\sigma}\\odot\\mathbf{\\epsilon}+\\mathbf{\\mu}$ makes the calculation of gradients easy\n",
    "    \n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}_{q}\\left[\\log p(\\mathbf{x},\\mathbf{z})\\right] & = & \\int p(\\mathbf{\\epsilon})\\log p(\\mathbf{x},\\mathbf{\\sigma}\\odot\\mathbf{\\epsilon}+\\mathbf{\\mu}) d\\mathbf{\\epsilon}, \\\\\n",
    "\\partial_{\\gamma}\\mathbb{E}_{q}\\left[\\log p(\\mathbf{x},\\mathbf{z})\\right] & = & \\int p(\\mathbf{\\epsilon})\\partial_{\\gamma}\\log p(\\mathbf{x},\\mathbf{\\sigma}\\odot\\mathbf{\\epsilon}+\\mathbf{\\mu}) d\\mathbf{\\epsilon}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "* Monte Carlo sampling of the gradient (its expectation it the true gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Three steps for VI in PyMC3\n",
    "1. Define model\n",
    "\n",
    "2. Run optimization\n",
    "\n",
    "3. Draw samples from variational posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Hierarchical regression\n",
    "<div style=\"text-align: center;\">\n",
    "<div style=\"display: inline-block; vertical-align: middle;\">\n",
    "<img src=\"./hierarchical_regression.png\" width=500 height=500 /></div> \n",
    "</div>\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{c} &\\sim & {\\cal N}(\\mu_{\\alpha}, \\sigma_{\\alpha}^{2}) \\\\\n",
    "\\beta_{c} &\\sim & {\\cal N}(\\mu_{\\beta}, \\sigma_{\\beta}^{2}) \\\\\n",
    "radon_{i,c} & = & \\alpha_{c} + \\beta_{c} * floor_{i,c} + \\epsilon_{c}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as hierarchical_model:\n",
    "    mu_a = pm.Normal('mu_alpha', mu=0., sd=100**2)\n",
    "    sigma_a = pm.Uniform('sigma_alpha', lower=0, upper=100)\n",
    "    mu_b = pm.Normal('mu_beta', mu=0., sd=100**2)\n",
    "    sigma_b = pm.Uniform('sigma_beta', lower=0, upper=100)\n",
    "    \n",
    "    a = pm.Normal('alpha', mu=mu_a, sd=sigma_a, shape=n_counties)\n",
    "    b = pm.Normal('beta', mu=mu_b, sd=sigma_b, shape=n_counties)\n",
    "    \n",
    "    eps = pm.Uniform('eps', lower=0, upper=100)\n",
    "    \n",
    "    radon_est = a[county_idx] + b[county_idx] * data.floor.values\n",
    "    \n",
    "    radon_like = pm.Normal('radon_like', mu=radon_est, sd=eps, observed=data.log_radon)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Run optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with hierarchical_model:\n",
    "    v_params = pm.variational.advi(n=100000)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Draw samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with hierarchical_model:\n",
    "    trace = pm.variational.sample_vp(v_params, draws=5000)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Bayesian neural network\n",
    "http://twiecki.github.io/blog/2016/06/01/bayesian-deep-learning/\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{W}_{0/1/2} & \\sim & {\\cal N}(0, 1) \\\\\n",
    "\\mathbf{h}_{0} & = & \\mathbf{x} \\\\\n",
    "\\mathbf{h}_{i} & = & {\\rm sigm}(\\mathbf{W}_{i-1}\\cdot\\mathbf{h}_{i-1}) \\\\\n",
    "\\mathbf{y} & \\sim & {\\cal N}({\\rm sigm}(\\mathbf{W}_{2}\\cdot\\mathbf{h}_{2}))\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as neural_network:\n",
    "    # Weights from input to hidden layer\n",
    "    weights_in_1 = pm.Normal('w_in_1', 0, sd=1, shape=(X.shape[1], n_hidden), \n",
    "                             testval=init_1)\n",
    "    \n",
    "    # Weights from 1st to 2nd layer\n",
    "    weights_1_2 = pm.Normal('w_1_2', 0, sd=1, shape=(n_hidden, n_hidden), \n",
    "                            testval=init_2)\n",
    "    \n",
    "    # Weights from hidden layer to output\n",
    "    weights_2_out = pm.Normal('w_2_out', 0, sd=1, shape=(n_hidden,), \n",
    "                              testval=init_out)\n",
    "    \n",
    "    # Build neural-network using tanh activation function\n",
    "    act_1 = T.tanh(T.dot(ann_input, weights_in_1))\n",
    "    act_2 = T.tanh(T.dot(act_1, weights_1_2))\n",
    "    act_out = T.nnet.sigmoid(T.dot(act_2, weights_2_out))\n",
    "    \n",
    "    # Binary classification -> Bernoulli likelihood\n",
    "    out = pm.Bernoulli('out', act_out, observed=ann_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Latent variables for observations\n",
    "* Consider the case where each of i.i.d. observations latent variables\n",
    "\n",
    "$$\n",
    "\\log p(\\mathbf{X},\\mathbf{Z},\\Theta) = \\sum_{i=1}^{N}\\log p(\\mathbf{x}_{i},\\mathbf{z}_{i},\\Theta)\n",
    "$$\n",
    "\n",
    "* Two types of latent variables\n",
    "\n",
    "    * $\\mathbf{z}_{i}$: Local random variables, related to each sample\n",
    "    * $\\Theta$: Global random variables, shared with all samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Autoencoding variational Bayes (1/2)\n",
    "* Approximate $p(\\Theta,\\mathbf{Z}|\\mathbf{X})$ by $q(\\Theta)\\prod_{i=1}^{N}q(\\mathbf{z}_{i})$\n",
    "* Assignment of variational parameters for each $\\mathbf{z}_{i}$ does not generalize to new data\n",
    "* Estimate the parameters of $q(\\mathbf{z}_{i})$ (means and stds) by neural network with parameters $\\nu$\n",
    "* Log likelihood $p(\\mathbf{x}_{i}|\\mathbf{z}_{i},\\Theta)$ can also parametrized ($\\eta$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Autoencoding variational Bayes (2/2)\n",
    "* Optimize ELBO wrt $\\gamma$, $\\nu$, and $\\eta$\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\gamma,\\nu,\\eta) & = & \n",
    "    \\mathbf{c}_{o}\\mathbb{E}_{q(\\Theta)}\\left[\n",
    "        \\sum_{i=1}^{N}\\mathbb{E}_{q(\\mathbf{z}_{i})}\\left[\n",
    "            \\log p(\\mathbf{y}_{i}|\\mathbf{z}_{i},\\Theta,\\eta)\n",
    "        \\right]\n",
    "    \\right] \\\\\n",
    "    && - \\mathbf{c}_{g}KL\\left[q(\\Theta)||p(\\Theta)\\right]\n",
    "       - \\mathbf{c}_{l}\\sum_{i=1}^{N}KL\\left[q(\\mathbf{z}_{i})||p(\\mathbf{z}_{i})\\right]\n",
    "\\end{eqnarray}\n",
    "\n",
    "* $\\mathbf{c}_{g/l/o}$ weighting constants\n",
    "* $\\mathbf{c}_{l/o}=N/M$ for mini-batches with size $M$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Convolutional variational autoencoder\n",
    "https://taku-y.github.io/notebook/20161105/convolutional_vae_keras_advi.html\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\tilde{\\mathbf{x}} & \\sim & {\\cal N}(f(g(\\mathbf{x})), 1)\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Hidden variables\n",
    "    zs = pm.Normal('zs', mu=0, sd=1, shape=(minibatch_size, dim_hidden), dtype='float32')\n",
    "\n",
    "    # Decoder and its parameters\n",
    "    dec = Decoder(zs, net=cnn_dec)\n",
    "\n",
    "    # Observation model\n",
    "    xs_ = pm.Normal('xs_', mu=dec.out.ravel(), sd=1, observed=xs_t.ravel(), dtype='float32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def cnn_enc(xs, latent_dim, nb_filters=64, nb_conv=3, intermediate_dim=128):\n",
    "    input_layer = InputLayer(input_tensor=xs,\n",
    "                             batch_input_shape=xs.get_value().shape)\n",
    "    model = Sequential()\n",
    "    model.add(input_layer)\n",
    "\n",
    "    cp1 = {'border_mode': 'same', 'activation': 'relu'}\n",
    "    cp2 = {'border_mode': 'same', 'activation': 'relu', 'subsample': (2, 2)}\n",
    "    cp3 = {'border_mode': 'same', 'activation': 'relu', 'subsample': (1, 1)}\n",
    "    cp4 = cp3\n",
    "\n",
    "    model.add(Convolution2D(1, 2, 2, **cp1))\n",
    "    model.add(Convolution2D(nb_filters, 2, 2, **cp2))\n",
    "    model.add(Convolution2D(nb_filters, nb_conv, nb_conv, **cp3))\n",
    "    model.add(Convolution2D(nb_filters, nb_conv, nb_conv, **cp4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(intermediate_dim, activation='relu'))\n",
    "    model.add(Dense(2 * latent_dim))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Limitation\n",
    "* Cannot handle discrete variables\n",
    "\n",
    "    * Marginalization of discrete variables (Gaussian mixture, latent dirichlet allocation)\n",
    "\n",
    "* Limited flexibility of the posterior approximation because of the mean-field approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Future work\n",
    "* Normalizing flows ([#1490](https://github.com/pymc-devs/pymc3/pull/1490))\n",
    "* Stein variational gradient descent ([#1549](https://github.com/pymc-devs/pymc3/pull/1549))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
