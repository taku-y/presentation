{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0\n",
    "%matplotlib inline\n",
    "\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.expanduser('~/git/github/pymc-devs/pymc3'))\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import daft\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "import IPython.core.display as display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br />\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<font size=\"7\"><b>Variational inference in </b></font>\n",
    "<div style=\"display: inline-block; vertical-align: middle;\">\n",
    "<img src=\"./pymc3_logo.jpg\" width=200 height=200 /></div> \n",
    "</div>\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<div style=\"text-align: right;\">\n",
    "<font size=\"6\">Taku Yoshioka</font>\n",
    "</div>\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyMC3\n",
    "* Python library for Bayesian inference\n",
    "\n",
    "    * Define probabilistic models consisting of random variables (RVs)\n",
    "    * Draw samples of RVs from the posterior distribution with MCMC\n",
    "    * Fit parametrized approximate posterior to data with variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "* Bayesian inference and variational inference\n",
    "* Stochastic gradient for optimization\n",
    "* Example: Bayesian neural network\n",
    "* Autoencoding Variational Bayes (AEVB)\n",
    "* Example: Convolutional variational autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic model\n",
    "* Decomposition of the joint probability of all random variables (RVs)\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x},\\mathbf{z}) = p(\\mathbf{x}|\\mathbf{z})p(\\mathbf{z})\n",
    "$$\n",
    "* $p(\\mathbf{x},\\mathbf{z})$: joint distribution of $\\mathbf{x}$ and $\\mathbf{z}$\n",
    "* $p(\\mathbf{z})$: prior distribution on $\\mathbf{z}$\n",
    "* $p(\\mathbf{x}|\\mathbf{z})$: likelihood of data $\\mathbf{x}$ given $\\mathbf{z}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian inference\n",
    "* Infer the posterior distribution $p(\\mathbf{z}|\\mathbf{x})$, the distribution of the unkown RVs $\\mathbf{z}$ given observations $\\mathbf{x}$ (which is also RVs), based on the Bayes theorem:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{z}|\\mathbf{x}) = \\frac{p(\\mathbf{x},\\mathbf{z})}{p(\\mathbf{x})}=\\frac{p(\\mathbf{x}|\\mathbf{z})p(\\mathbf{z})}{p(\\mathbf{x})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variational inference\n",
    "* Approximate $p(\\mathbf{z}|\\mathbf{x})$ by variational distribution $q(\\mathbf{z})$\n",
    "* Maximize evidence lower bound (ELBO) ${\\cal L}(q)$ w.r.t. $q$ minimizes $KL[q(\\mathbf{z})||p(\\mathbf{z}|\\mathbf{x})]$\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\cal{L}(q) & = & \\mathbb{E}_{q(\\mathbf{z})}\\left[\\log p(\\mathbf{x},\\mathbf{z}) - \\log q(\\mathbf{z})\\right] \\\\\n",
    "           & = & \\mathbb{E}_{q(\\mathbf{z})}\\left[\\log p(\\mathbf{x}|\\mathbf{z})\\right] - KL\\left[\\log q(\\mathbf{z})||p(\\mathbf{z})\\right] \\\\\n",
    "           & = & \\log p(\\mathbf{x}) + KL[q(\\mathbf{z})||p(\\mathbf{z}|\\mathbf{x})]\n",
    "\\end{eqnarray}\n",
    "\n",
    "* Pros: Can obtain tractable solution under factorized $q(\\mathbf{x})$ and conjugate prior\n",
    "* Cons: Restricted probabilistic models because of the tractability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stochastic gradient\n",
    "* Normal distribution as $q(\\mathbf{z})$ with parameters $\\gamma$ (variational parameters)\n",
    "\n",
    "$$\n",
    "q_{\\lambda}(\\mathbf{z}) = \\prod_{d=1}^{D}N(z_{d};\\mu_{d},\\sigma_{d}^{2}), \\gamma=\\left\\{\\mu_{d}, \\sigma_{d}\\right\\}_{d=1}^{D}\n",
    "$$\n",
    "\n",
    "* Maximize ELBO wrt $\\gamma$ with stochastic gradient\n",
    "* Reparametrization $\\mathbf{z} = \\mathbf{\\sigma}\\odot\\mathbf{\\epsilon}+\\mathbf{\\mu}$ makes the calculation of gradients easy\n",
    "    \n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}_{q}\\left[\\log p(\\mathbf{x},\\mathbf{z})\\right] & = & \\int p(\\mathbf{\\epsilon})\\log p(\\mathbf{x},\\mathbf{\\sigma}\\odot\\mathbf{\\epsilon}+\\mathbf{\\mu}) d\\mathbf{\\epsilon}, \\\\\n",
    "\\partial_{\\gamma}\\mathbb{E}_{q}\\left[\\log p(\\mathbf{x},\\mathbf{z})\\right] & = & \\int p(\\mathbf{\\epsilon})\\partial_{\\gamma}\\log p(\\mathbf{x},\\mathbf{\\sigma}\\odot\\mathbf{\\epsilon}+\\mathbf{\\mu}) d\\mathbf{\\epsilon}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "* Monte Carlo sampling of the gradient (its expectation it the true gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three steps for VI in PyMC3\n",
    "1. Define model\n",
    "    * aaa\n",
    "\n",
    "\n",
    "2. Run optimization\n",
    "    * bbb\n",
    "\n",
    "\n",
    "3. Draw samples from variational posterior\n",
    "    * ccc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Bayesian neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent variables for observations\n",
    "* Consider the case where each of i.i.d. observations latent variables\n",
    "\n",
    "$$\n",
    "\\log p(\\mathbf{X},\\mathbf{Z},\\Theta) = \\sum_{i=1}^{N}\\log p(\\mathbf{x}_{i},\\mathbf{z}_{i},\\Theta)\n",
    "$$\n",
    "\n",
    "* Two types of latent variables\n",
    "\n",
    "    * $\\mathbf{z}_{i}$: Local random variables, related to each sample\n",
    "    * $\\Theta$: Global random variables, shared with all samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoding variational Bayes\n",
    "* Approximate $p(\\Theta,\\mathbf{Z}|\\mathbf{X})$ by $q(\\Theta)\\prod_{i=1}^{N}q(\\mathbf{z}_{i})$\n",
    "* Assignment of variational parameters for each $\\mathbf{z}_{i}$ does not generalize to new data\n",
    "* Estimate the parameters of $q(\\mathbf{z}_{i})$ (means and stds) by neural network with parameters $\\nu$\n",
    "* Log likelihood $p(\\mathbf{x}_{i}|\\mathbf{z}_{i},\\Theta)$ can also parametrized (\\eta)\n",
    "* Optimize ELBO wrt $\\gamma$, $\\nu$, and $\\eta$\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\cal L}(\\gamma,\\nu,\\eta) & =\n",
    "    \\mathbf{c}_{o}\\mathbb{E}_{q(\\Theta)}\\left[\n",
    "        \\sum_{i=1}^{N}\\mathbb{E}_{q(\\mathbf{z}_{i})}\\left[\n",
    "            \\log p(\\mathbf{y}_{i}|\\mathbf{z}_{i},\\Theta,\\eta)\n",
    "        \\right]\n",
    "    \\right]\n",
    "    - \\mathbf{c}_{g}KL\\left[q(\\Theta)||p(\\Theta)\\right]\n",
    "    - \\mathbf{c}_{l}\\sum_{i=1}^{N}KL\\left[q(\\mathbf{z}_{i})||p(\\mathbf{z}_{i})\\right]\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Convolutional variational autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future work\n",
    "* Normalizing flows ([#1490](https://github.com/pymc-devs/pymc3/pull/1490))\n",
    "* Stein variational gradient descent ([#1549](https://github.com/pymc-devs/pymc3/pull/1549))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
